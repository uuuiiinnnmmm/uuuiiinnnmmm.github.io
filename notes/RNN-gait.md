---
layout: page
title: 深度学习笔记：RNN模型深度解析
permalink: /notes/RNN-gait/
---

本文是我的“下肢步态识别”研究项目中所涉及核心算法的详细梳理与学习笔记。项目中，我们旨在解决一个典型的时间序列分类问题，这个过程促使我对几种主流的深度学习模型进行了深入的对比分析。

*   **目录**
{:toc}

---
### 1. 基础模型：连续时间循环神经网络 (CTRNN)

CTRNN的状态不是在离散的时间步上更新，而是由一组常微分方程（ODEs）描述的。

其最基础的神经元动态可以表示为：
$$
\tau_i \frac{du_i}{dt} = -u_i + \sum_{j=1}^{N} w_{ji} y_j + I_i
$$
$$
y_i = \sigma(u_i + b_i)
$$

*   **变量说明**:
    *   $u_i$: 第 $i$ 个神经元的内部状态（膜电位）。
    *   $y_i$: 第 $i$ 个神经元的输出（激活值）。
    *   $\tau_i$: 第 $i$ 个神经元的时间常数，控制状态变化的快慢。$\tau$ 越大，状态变化越慢。
    *   $w_{ji}$: 从神经元 $j$到神经元 $i$ 的连接权重。
    *   $I_i$: 外部输入到神经元 $i$ 的信号。
    *   $b_i$: 神经元 $i$ 的偏置。
    *   $\sigma(\cdot)$: 激活函数，通常是 sigmoid 或 tanh 函数。

**核心思想**: $-u_i$ 这一项代表了“泄露”（leak），即在没有输入的情况下，神经元的状态会逐渐衰减到0。因此，CTRNN也被称为“漏积分”（leaky integrator）模型。它的状态是过去所有输入的加权积分，但权重会随时间指数衰减。

---

### 2. 模型一：用于学习随机时序的随机-确定性CTRNN (Stochastic-Deterministic CTRNN)

该模型源于论文 ****，其核心目标是让机器人不仅能学习和复现确定性的运动模式，还能模仿和生成具有内在随机性、不断波动的时序信号（如人类的示范动作）。

#### **核心概念**
标准的RNN是确定性的：给定相同的初始状态和输入，它总会产生完全相同的输出序列。为了生成不确定的、波动的行为，这篇论文提出的模型（通常被称为 **Stochastic CTRNN** 或 **S-CTRNN**）将网络的输出解释为 **一个概率分布的参数**，而不是一个确定的值。

#### **数学原理与公式**

1.  **网络动态 (与CTRNN类似)**:
    网络内部的神经元动态仍然遵循CTRNN的原则，即通过常微分方程更新状态。这部分负责学习时间序列的确定性（或称“结构性”）部分。
    $$
    \tau_i \frac{du_i(t)}{dt} = -u_i(t) + \sum_{j=1}^{N} w_{ji} y_j(t) + \sum_{k=1}^{M} v_{ki} I_k(t)
    $$
    $$
    y_i(t) = \sigma(u_i(t))
    $$
    这里 $v_{ki}$ 是从第 $k$ 个外部输入 $I_k$ 到第 $i$ 个神经元的权重。

2.  **随机输出层 (核心创新)**:
    网络的输出层并不直接输出预测值，而是输出一个高斯分布的均值 $\mu(t)$ 和标准差 $\sigma(t)$。
    $$
    \mu(t) = \sum_{j=1}^{N} w_j^{\mu} y_j(t)
    $$
    $$
    \beta(t) = \sum_{j=1}^{N} w_j^{\beta} y_j(t)
    $$
    $$
    \sigma(t) = \exp(\beta(t))
    $$
    *   **解释**:
        *   网络用其隐藏状态 $y_j(t)$ 的线性组合来预测下一时刻的均值 $\mu(t)$。
        *   同时，用另一组权重 $w_j^{\beta}$ 来预测一个对数值 $\beta(t)$，然后通过指数函数 $\exp(\cdot)$ 来确保标准差 $\sigma(t)$ 始终为正。

3.  **损失函数 (训练目标)**:
    模型的训练目标不再是最小化均方误差（MSE），而是最大化观测数据的对数似然，等价于最小化 **负对数似然 (Negative Log-Likelihood, NLL)**。对于高斯分布，损失函数为：
    $$
    \text{Loss} = \frac{1}{2} \sum_t \left( \log(2\pi\sigma(t)^2) + \frac{(\text{target}(t) - \mu(t))^2}{\sigma(t)^2} \right)
    $$
    *   **解释**: 这个损失函数会同时惩罚两件事：
        *   **均值误差**: 当预测的均值 $\mu(t)$ 远离目标值 $\text{target}(t)$ 时，第二项会变大。
        *   **不确定性**: 当网络的预测不确定性 $\sigma(t)$ 过高时，第一项 $\log(\sigma(t)^2)$ 会变大。反之，如果网络对某个预测非常自信（$\sigma(t)$ 很小）但预测错了，第二项的分母会变得很小，导致损失急剧增大。

**总结**: 该模型通过让RNN学习预测一个概率分布，成功地将确定性动态（RNN内部状态演化）与随机性（从预测的分布中采样）结合起来，使其能够生成丰富且自然的波动行为。

---

### 3. 模型二：门控自适应CTRNN (Gated Adaptive CTRNN)

该模型源于论文 ****，旨在将现代深度学习中强大的门控机制（如LSTM/GRU）和自适应能力引入到CTRNN框架中，以增强其学习复杂时间尺度和长程依赖的能力。

#### **核心概念**
标准的CTRNN中，每个神经元的时间常数 $\tau$ 是固定的超参数。这限制了网络的灵活性，因为真实世界的数据可能在不同时间尺度上包含信息。该模型通过引入 **自适应时间尺度** 和 **类LSTM的门控机制** 来解决这个问题。

#### **数学原理与公式**

该模型（我们称之为 **GACTRNN**）为每个神经元定义了连续时间的输入门 $i(t)$ 和遗忘门 $f(t)$。

1.  **自适应时间常数**:
    每个神经元的时间常数 $\tau_i$ 不再是固定的，而是可学习的参数。通常通过学习一个无约束的参数 $\eta_i$ 并应用指数函数来保证其为正：
    $$
    \tau_i = \exp(\eta_i)
    $$

2.  **门控动态**:
    输入门 $i_j(t)$ 和遗忘门 $f_j(t)$ 的动态由以下公式描述，它们控制着信息的流入和保留：
    $$
    i_j(t) = \sigma\left(\sum_{k} w_{kj}^{ix} x_k(t) + \sum_{l} w_{lj}^{ih} h_l(t-\Delta t) + b_j^i\right)
    $$
    $$
    f_j(t) = \sigma\left(\sum_{k} w_{kj}^{fx} x_k(t) + \sum_{l} w_{lj}^{fh} h_l(t-\Delta t) + b_j^f\right)
    $$
    *   这些门的计算方式与离散时间的GRU/LSTM非常相似，它们的值取决于当前输入 $x(t)$ 和前一时刻的隐藏状态 $h(t-\Delta t)$。

3.  **神经元状态更新**:
    神经元的内部状态 $u_i(t)$ 更新方程被门控机制所调节：
    $$
    \tau_i \frac{du_i(t)}{dt} = f_i(t) \odot (-u_i(t)) + i_i(t) \odot \left( \sum_{j=1}^{N} w_{ji} \sigma(u_j(t)) + I_i(t) \right)
    $$
    *   **解释**:
        *   **遗忘门 $f_i(t)$**: 它与“泄露”项 $-u_i(t)$ 相乘。如果 $f_i=1$，神经元正常“遗忘”；如果 $f_i=0$，则“泄露”被完全阻止，信息被强制保留。
        *   **输入门 $i_i(t)$**: 它控制着新信息（来自其他神经元和外部输入的加权和）流入的强度。

**总结**: GACTRNN将CTRNN的连续动态特性与LSTM/GRU强大的信息流控制能力相结合。通过可学习的、异构的时间常数 $\tau_i$ 和连续时间的门控，该模型能够让网络的不同部分以不同的速度运作，从而更有效地捕捉多尺度的时间依赖关系。

---

### 4. 模型三：预测编码RNN (Predictive Coding RNN)

该模型源于论文 ****，它基于计算神经科学中的“预测编码”理论。其核心思想是，大脑（或模型）并非被动地处理感官输入，而是主动地生成关于外部世界的预测，并不断地根据 **预测误差** 来修正其内部表征。

#### **核心概念**
在一个分层的RNN中，高层网络不断地预测低层网络的活动状态。神经元的更新不再仅仅依赖于前馈输入，而是主要由 **自上而下的预测** 和 **自下而上的预测误差** 之间的相互作用驱动。

#### **数学原理与公式**

该模型通常包含至少两个层次：一个较低的层次 $p_1$ 和一个较高的层次 $p_2$。

1.  **预测与误差计算**:
    *   **自上而下的预测**: 高层 $p_2$ 试图预测低层 $p_1$ 的状态。这个预测表示为 $p'_1$。
        $$
        p'_1 = W_{21} \cdot p_2
        $$
    *   **预测误差**: 误差 $\varepsilon_1$ 是低层的实际状态 $p_1$ 和来自高层的预测 $p'_1$ 之间的差异。
        $$
        \varepsilon_1 = p_1 - p'_1
        $$

2.  **状态更新动态 (核心)**:
    每个神经元的状态更新都致力于最小化预测误差。这可以通过梯度下降来实现。状态 $p_1$ 和 $p_2$ 的变化率（导数）与误差的梯度成正比。
    $$
    \tau_1 \frac{dp_1}{dt} = - \frac{\partial E}{\partial p_1} = - \varepsilon_1 + W^T_{21} \varepsilon_2
    $$
    $$
    \tau_2 \frac{dp_2}{dt} = - \frac{\partial E}{\partial p_2} = - \varepsilon_2
    $$
    *   其中 $E$ 是一个能量函数，通常是误差的平方和 $E = \frac{1}{2}\sum_i \varepsilon_i^2$。
    *   $\varepsilon_2$ 是更高一层的误差信号。
    *   **解释**:
        *   低层状态 $p_1$ 的更新有两个驱动力：一个是修正自身的误差（$-\varepsilon_1$），另一个是传递来自更高层的误差信号（$W^T_{21} \varepsilon_2$）。
        *   高层状态 $p_2$ 的更新则致力于减少它自己的预测给下一层带来的误差。

3.  **与外部输入的交互**:
    当模型与外部世界（如感觉输入 $s$）交互时，最底层的误差是其对感觉输入的预测 $g(p_1)$ 与真实感觉 $s$ 之间的差异。
    $$
    \varepsilon_s = s - g(p_1)
    $$
    这个感觉误差会驱动最底层网络 $p_1$ 的更新，然后逐层向上传播。

**总结**: 预测编码RNN改变了传统RNN的信息流。信息不再是简单地自下而上传递，而是在一个由自上而下的预测和自下而上的误差信号构成的循环中不断迭代和修正。这种机制被认为更接近生物大脑处理信息的方式，并使模型能够学习到数据的生成模型。
