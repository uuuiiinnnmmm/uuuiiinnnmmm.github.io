---
layout: page
title: 机械臂抓取引入深度学习的初尝试
permalink: /notes/ROS2-Moveit2-Gazebo-UR5e-catching/
---

本文是我学习并尝试将深度学习用到物体抓取上，我首先学习了一个基于ROS2和Moveit2搭建的项目，接着试图在其上面搭建一个深度学习模型。

*   **目录**
{:toc}

---

# UR5e 动态抓取系统学习笔记

## 零、项目概述

本项目是一个基于 ROS 2 和 MoveIt2 的完整机器人抓取系统。其核心目标是让 UR5e 机械臂能够利用深度相机“看到”环境，通过深度学习模型“思考”最佳抓取点，并由 MoveIt2“规划”出安全、无碰撞的路径来执行抓取任务。这是一个从底层模型、中层控制到上层智能应用的综合性项目。

## 一、第一阶段：机器人的数字生命 —— 模型与描述

学习的第一步是理解机器人如何在数字世界中被“创造”和“描述”。这是后续一切仿真、规划和控制的基础。

> **核心思想**：在 ROS 中，一切操作都基于一个精确的机器人模型。这个模型就是连接虚拟与现实的桥梁。

### 1.1 关键功能包

*   `robotiq_description`: 描述 Robotiq 手爪的物理属性。
*   `ur5e_gripper_description`: 将 UR5e 机械臂和 Robotiq 手爪的模型**组合**成一个完整的机器人。

### 1.2 文件夹结构与作用

通过分析 `robotiq_description`，我们理解了机器人描述包的标准构成：

| 文件夹 | 核心用途 | 关键内容 | 理解 |
| :--- | :--- | :--- | :--- |
| **`urdf/`** | **定义骨架** | `.xacro`, `.urdf` | 机器人的核心蓝图，描述连杆、关节和物理结构。XACRO 是 URDF 的“高级版”，支持宏和变量，实现了模型的模块化。 |
| **`meshes/`** | **赋予外观** | `.stl`, `.dae` | 存放机器人各个部件的 3D 模型文件，被 URDF 文件引用，用于可视化和碰撞检测。 |
| **`launch/`** | **提供测试脚本** | `.launch.py` | 用于一键启动相关节点，方便在 RViz2 中快速预览和调试模型。 |
| **`rviz/`** | **固化视图** | `.rviz` | 保存 RViz2 的配置，确保每次启动都能看到统一、已配置好的调试界面。 |

### 1.3 学习要点

*   **URDF vs XACRO**: URDF 是最终被系统解析的标准格式，而 XACRO 是我们用来编写和维护复杂模型的更高效的工具。
*   **Visual vs Collision**: `<visual>` 标签用于显示，追求逼真；`<collision>` 标签用于物理计算，追求高效。
*   **模块化思想**: 独立的机械臂和手爪可以有各自的 `description` 包，再由一个顶层包通过 XACRO 的 `include` 指令将它们“焊接”在一起。

## 二、第二阶段：让机器人动起来 —— 启动与仿真

模型只是静态的描述，我们需要 `launch` 文件来启动节点，搭建一个让模型可以“存活”和“运动”的环境。

> **核心思想**：Launch 文件是 ROS 2 系统的“总指挥”，它负责按照预定的“剧本”（LaunchDescription）来启动、配置和连接各个节点，搭建起一个完整的运行环境。

### 2.1 从可视化到仿真

我们分析了两种不同目标的 `launch` 文件：

1.  **纯可视化 (`view_gripper.launch.py`)**:
    *   **目标**: 在 RViz2 中查看和调试模型。
    *   **“黄金三角”**:
        1.  `joint_state_publisher_gui`: 提供一个 GUI 滑块，**手动**发布关节状态到 `/joint_states` 话题。
        2.  `robot_state_publisher`: 订阅 `/joint_states`，结合 URDF 模型，计算并发布 TF 坐标变换。
        3.  `rviz2`: 订阅 TF，将模型可视化地呈现出来。
    *   这是一个**开环**的、纯粹用于展示的系统。

2.  **物理仿真 (`gazebo.launch.py`)**:
    *   **目标**: 在 Gazebo 物理引擎中运行和控制模型。
    *   **核心组件**:
        1.  `gazebo_ros`: 启动 `gzserver` (后端物理引擎) 和 `gzclient` (前端图形界面)。
        2.  `spawn_entity`: 将 URDF 模型从参数服务器“投放”到 Gazebo 世界中。
        3.  `ros2_control`: **关键的“神经系统”**。通过 Gazebo 插件，连接了 ROS 的控制指令和 Gazebo 的物理关节。
    *   **控制器 (`spawner`)**:
        *   `joint_state_broadcaster`: **从 Gazebo 中读取**真实关节状态，并发布到 `/joint_states`。这是**闭环**控制的基础。
        *   `..._controller` (如 `ur5e_arm_controller`): **订阅**控制指令话题，并将指令**写入**到 Gazebo 的仿真关节中，驱动模型运动。

### 2.2 学习要点

*   `generate_launch_description()` 函数的本质是返回一个**启动计划的“蓝图” (`LaunchDescription`)**，而不是执行命令或返回状态。
*   Launch 系统提供了丰富的**动作 (Actions)**，如 `Node`, `IncludeLaunchDescription`, `DeclareLaunchArgument`, `RegisterEventHandler` 等，可以实现非常灵活和复杂的启动逻辑。

## 三、第三阶段：赋予智慧之脑 —— MoveIt2 运动规划

模型能在仿真中被控制了，但如何让它**自主地、智能地**运动呢？这就是 MoveIt2 的用武之地。

> **核心思想**：MoveIt2 在 URDF（身体）的基础上，通过 SRDF（灵魂）来理解机器人的运动学语义，从而实现复杂的路径规划、碰撞检测和运动学求解。

### 3.1 SRDF：机器人的“使用说明书”

*   **URDF vs SRDF**:
    *   **URDF**: 描述“是什么”（物理结构）。
    *   **SRDF (Semantic Robot Description Format)**: 描述“怎么用”（语义信息）。
*   **SRDF 的核心内容**:
    *   **规划组 (Planning Groups)**: 定义哪些关节共同组成一个“手臂”或“手爪”。这是规划的基本单位。
    *   **末端执行器 (End-Effectors)**: 特殊的规划组，通常指手爪。
    *   **预定义位姿 (Named Poses)**: 为规划组定义一些常用的姿态，如手爪的 `open` 和 `close` 状态。
    *   **碰撞免除 (Disabled Collisions)**: 告诉规划器哪些连杆之间永远不会碰撞（如相邻连杆），以大幅提高规划效率。

### 3.2 学习要点

*   `robotiq_moveit_config` 这样的包体现了**配置的模块化**。它只提供与手爪相关的 SRDF“配置插件”，可以被任何集成了该手爪的机械臂项目所复用。
*   MoveIt2 的核心节点 `move_group` 在启动时，会同时加载 `/robot_description` (URDF) 和 `/robot_description_semantic` (SRDF)，在内存中构建一个完整的、可用于规划的机器人模型。

## 四、第四阶段：迈向真正智能 —— 集成深度学习

这是项目的升华，将传统的自动化流程与现代 AI 技术相结合。

> **核心思想**：深度学习负责**感知与决策（抓哪里最好？）**，MoveIt2 负责**规划与执行（如何安全地过去？）**。两者各司其职，相辅相成。

### 4.1 系统架构升级

在原有系统基础上，我们需要增加三个新的应用层节点：

1.  **相机驱动节点**: 负责发布原始的图像和点云数据，作为系统的“眼睛”。
2.  **深度学习抓取节点**:
    *   **输入**: 订阅图像/点云数据。
    *   **处理**: 内部运行一个预训练的抓取检测模型（如 GG-CNN），推理出场景中最佳的抓取姿态。
    *   **输出**: 通过一个 ROS 2 服务或动作，向外提供计算出的最佳抓取目标。
3.  **主控制/应用节点**:
    *   **职责**: 扮演总指挥，串联整个流程。
    *   **流程**:
        1.  调用**深度学习节点**的服务，请求一个最优抓取姿态（该姿态是相对于相机坐标系的）。
        2.  使用 `tf2_ros` 进行**坐标变换**，将抓取姿态转换到 MoveIt2 需要的机器人基坐标系 (`base_link`) 下。
        3.  调用 **MoveIt2** 的接口，将变换后的姿态作为目标，进行规划和运动。
        4.  控制**手爪控制器**完成最终的抓取动作。

### 4.2 学习要点

*   AI 的集成是在**应用层**完成的，它为底层的机器人控制与规划系统提供了一个**智能的目标源**。
*   **TF2 坐标变换**在连接不同模块（相机、机器人、AI模型）时扮演着至关重要的角色。

---

## 总结与展望

通过这个项目的学习，我不仅掌握了 ROS 2 和 MoveIt2 的核心使用方法，更重要的是建立了一套完整的机器人系统开发思维：从**底层模型**的精确描述，到**中层仿真与控制**的闭环搭建，再到**上层运动规划**的智能实现，最后通过**AI赋能**实现自主决策。

接下来的学习方向可以是：
1.  深入研究不同的抓取检测算法。
2.  学习 MoveIt2 Servo 实现实时轨迹跟踪和人机协作。
3.  将系统部署到真实的 UR5e 硬件上，应对真实世界的挑战。
